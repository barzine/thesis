\chapter{Available high-throughput normal human Datasets}
\label{ch:datasets}

\begin{comment}
\setlength{\epigraphwidth}{0.57\textwidth}
\setlength{\epigraphrule}{0.1pt}
\epigraph{Data! Data! Data! I can’t make bricks without clay!}{Sherlock Homes
    (Sir Arthur Conan Doyle)}
\end{comment}

In the past few years, many laboratories studied the expression
of the genes within normal humans at the transcriptome and at
the proteome levels. In this chapter, I review the data I use within my thesis
and how it has been processed.

When not stated otherwise, all the computational processing of the \Rnaseq\ part
described here have been performed by myself under the supervision of
the Dr Alvis Brazma. I also received general feedback from Dr Mar Gonzalez-Portà,
Dr Johan Rung and Dr Nuno Fonseca. The proteome data has been processed by
Dr James Wright from the Wellcome Trust Sanger Institute.


\section{Introduction}

Every dataset with which I worked is fitting three main criteria.
First, they comprise human normal samples from at least three kinds of tissues.
Secondly, they are non-targeted high-throughput \ie\ gene expression
quantifications are based on \Rnaseq\ for the transcriptome or on \ms\ for the
proteome\footnote{These technologies allow in theory to study the whole
repertoire of \glspl{RNA} or proteins in a sample.}.
Finally, the \emph{raw} data is available and reusable.

In the next section, I first describe the \Rnaseq\ and then the \ms\ data I use
in my thesis; then I detailed how these data have been processed to be
employed in the next chapters for various analyses that explore the transcriptome,
the proteome and the integration attempt of these two biological layers.


\section{RNA-Seq Transcriptome data}
\label{sec:rnaseq-data}

For my thesis, I retrieve as many available studies as I can get. There are a
few more studies that I wanted to use on the
transcriptomic side, but the reusability point was often the critical reason
why they have not been included.

Indeed, many times I encountered data with ambiguous encoding format. As
those studies were a bit outdated,
I was unable to get the information from the original authors either.
I describe hereafter the 5 transcriptomic datasets I used
in the chronological order of their first public release.
\Cref{tab:Trans5DF} summarises the main characteristics of these datasets.

\begin{sidewaystable}
           \centering
           \caption[General description of the 5 transcriptomic datasets
           (\Rnaseq) used for this study]{\label{tab:Trans5DF}\textbf{General
           description of the 5 transcriptomic datasets (\Rnaseq) used for this
           study}\\\footnotesize{Illumina Body Map (IBM) has not ``regular''
           technical replicates as the ``replicates'' are the product of
           different protocols,\\thus are unfit to estimate the specific noise of
           either protocol (single-end or paired-end).\\
           \NB The protocols used for \Gtex\ and Castle datasets are not the same:
           \\\Gtex\ is following the most common ribodepletion protocol, while\\
           Castle is based on  a targeted amplification protocol.\\}}
       \begin{tabular}{@{}cccccccccc@{}}
       \toprule
       \multicolumn{1}{c|}
           {\multirow{2}{*}{ArrayExpress ID}} &
            \multicolumn{1}{c|}{\multirow{2}{*}{Data ID}} &
            \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}Library\\Preparation\end{tabular}} &
            \multicolumn{2}{c|}{Sequencing} &
            \multicolumn{2}{c|}{Replicates} &
            \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Tissue\\
                    Number\end{tabular}}} &
            \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Multi-sampling\\ from the \\ same
            individual\end{tabular}} \\
            \cmidrule(lr){3-8}
            \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &
            \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Whole\\ RNA\end{tabular}} &
            \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}PolyA\\ selected\end{tabular}} &
            \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Single\\ end\end{tabular}} &
            \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Paired\\ end\end{tabular}} &
            \multicolumn{1}{c|}{Biological} & \multicolumn{1}{c|}{Technical} &
            \multicolumn{1}{c|}{} &  \\
       \midrule
       E-MTAB-305 & Castle & \mycheckmark &  & \mycheckmark &  &  &  & 11 &  \\
       E-GEOD-30352 & Brawand &  & \mycheckmark & \mycheckmark  &  &
           \mycheckmark  &  & 8 &  \\
       E-MTAB-513 & IBM &  & \mycheckmark & \mycheckmark & \mycheckmark &  &
           (\mycheckmark) & 16 &  \\
       E-MTAB-2836 \footnotesize{(and E-MTAB-1733)}& Uhlén &  & \mycheckmark &
           & \mycheckmark & \mycheckmark  & \mycheckmark  & 32 &  \\
       E-MTAB-2919 & Gtex (v4)  &  & \mycheckmark &  & \mycheckmark &
           \mycheckmark &  & 54 & \mycheckmark \\
       \bottomrule
       \end{tabular}
\end{sidewaystable}

\subsection{Castle et al. dataset}
\label{subsec:castlepresentation}
This dataset has been published along with the \paper{\citetitle{castleData}}
by \citet{castleData} who were interested in exploring the whole RNA repertoire
with sequencing-based technology. They essentially focused their study
on the non-coding part.

They used multiple-donors pooled tissues created from purchased total \gls{RNA}
samples and prepared the 11 libraries following a whole transcriptomic protocol
where nonribosomal \gls{RNA} transcripts are
specifically amplified by \gls{PCR} \mycite{Armour:2009}.

They generated an average of 50 million sequence reads per tissue
using an Illumina Genome Analyser-II sequencer (single-end).
They trimmed their original reads to 28 \gls{nt}
and then released them through EMBL archives (\ENA{ERP000257}
and \ArrayExpress{E-MTAB-305}).

Despite several limitations (lack of replicates, old technology, small reads),
I used this dataset for two main reasons. First, it is the oldest available
\Rnaseq\ data I found that was performed on Human normal tissues. Thus, the
results congruence of this dataset to the others gives a rough assessment about
the extent of \Rnaseq\ datasets that can be integrated together in an atlas.
Secondly as \Rnaseq\ studies are prepared mainly with polyA-selected protocols
nowadays, I was interested to gauge how the library preparation
protocols --- and the presence of \glspl{ncRNA} --- can affect the
quantifications and then the final outcomes.


\subsection{Brawand et al. dataset}
\label{subsec:brawandpresentation}
In the corresponding article entitled \paper{\citetitle{VTpaper}},
\citet{VTpaper} focused their interest on the
evolution of the mammalian transcriptomes --- while there were existing studies
on the matter, the sequencing approach was then creating new perspectives.

They collected 6 organs from 10 different vertebrates:
9 mammalians (including Human) and a bird. There is no technical replicate
but there are two biological replicates per tissue:
one male and one female for every tissue but the testis (two males).
They used a polyA-selected protocol to prepare their 131 libraries (including 23
for \species{Homo sapiens}).
Hence, the samples are largely enriched in protein coding genes.

They generated an average of 3.2 billion reads of 76 base pairs per sample
using an Illumina Genome Analyser IIx (single-end) and they released them
through \gls{GEO} (accession number: GSE30352).
I personally retrieved the human data from
\ArrayExpress{E-GEOD-30352}\footnote{ArrayExpress routinely imports
datasets from \gls{GEO} on a weekly basis.}.


\subsection{Illumina Body Map 2.0}
\label{subsec:ibmpresentation}
This dataset has been first created in 2010 and released in
2011\footnote{See: \citetitle{ibmEnsembl} - \cite{ibmEnsembl}} by Illumina
mostly to advertise its most recent technology improvement at that time:
the paired-end sequencing. Indeed, until then, all the sequencing was done
from only one end of the \gls{DNA} or \gls{cDNA} fragments.\footnote{From that
date, most of the following transcriptome studies based on \Rnaseq\ are using
paired-end sequencing.}

It comprises 16 tissues (one donor per tissue) and was prepared with a
polyA-selected preparation protocol to enrich the libraries with protein
coding genes. Indeed, Illumina main incentive to develop its paired-end
technology was to improve the accurate identification of spliced \glspl{mRNA}.

Although each sample has been sequenced twice and that we have in principle
\emph{technical} replicates, these are not ``regular'' technical replicates.
\emph{Technical} replicates, by contrast to \emph{biological} replicates,
usually imply that their processing uses the same sample source and protocols.
Thus, the error and the noise due to a specific technique could be determined.
Here, however, each tissue has been sequenced once with a singled-end protocol
and once with a paired-end one to compare them together.

The sequencing was performed with an Illumina HiSeq 2000 and the reads were
released through \ArrayExpress{E-MTAB-503} (\ENA{ERP000546}), from where I
retrieved the single-end and paired-end mono-tissue samples.

Despite the lack of biological replicates, I used this dataset as it was for an
extended time the most extensive freely available \Rnaseq\ dataset of human
tissues and as such has been referenced many times since it was released.

\subsection{Uhlén et al. dataset}
\label{subsec:uhlenPresentation}
Uhlén et al.\ have created an atlas,
\href{http://www.proteinatlas.org/}{Human Protein Atlas}\footnote{%
Human Protein Atlas ---
\href{http://www.proteinatlas.org/}{http://www.proteinatlas.org/}},
revolving mostly around the spatial
distribution of the proteins through the Human body. They use many approaches
and techniques which also include \Rnaseq. They first released their \Rnaseq\
data of 27 normal tissues as a part of their article \paper{\citetitle{Uhlen2014}}
\mycite{Uhlen2014}. Later, they extended their dataset with new samples and 5 new
tissues. The latest version was published with \paper{\citetitle{Uhlen2015}}
\mycite{Uhlen2015} in \textit{Science}.

They provide (at least two) biological replicates for the 32 tissues.
Except for a very small number, the tissues have both male and female donors.
There are also technical replicates for many of the tissues. Two hundred samples
were picked after a screening of frozen biopsy samples by pathologists.

Then they prepared the libraries following a polyA-selected protocol and
paired-end sequenced them with an Illumina HiSeq 2000 or 2500. I first started
to work with the early version of this dataset
(\ArrayExpress{E-MTAB-1733} --- 171 samples for 27 tissues)
and then I upgraded my work with the extended more recent version
(\ArrayExpress{E-MTAB-2836}).

At the time of the redaction of this thesis, this normal human dataset is the
most important one that is also freely and directly available: either regarding
the number of tissues (see \Cref{tab:Trans5DF})
or the number of samples (see \Cref{tab:Lib5DF}).


\subsection{GTEx data}
\label{subsec:gtexPresentation}
The Genotype-Tissue Expression (\gls{GTEx}) project is funded by the \gls{NIH}
Common Fund and aims to establish, in its authors own words,
a resource database and associated tissue bank
for the study of the relationship between genetic variation and gene expression
and other molecular phenotypes in multiple reference tissues. The project was first
explained in a paper from the \cite{GTEx2013}. It consists to quickly collect
many tissues from postmortem donors so genotype-tissue expression analyses could
be done, notably \gls{eQTL} variants studies which study the modulation
of \gls{RNA} expression in function of \glspl{SNP}. The results of the
analyses are released through the GTEx portal\footnote{\Gtex\ portal --- %
\href{http://gtexportal.org}{http://gtexportal.org}}.

As the project is quite ambitious and the collection and sequencing of the samples
are taking time, several freezes of the data have been released. My work is
including samples up to the fourth release of the pilot phase (v4). This
release includes 54 tissue/cell types (53 normal and 1 tumour)
collected on 551 individuals for a total of 3,276 samples.

The libraries were prepared from whole tissue \gls{RNA} extracts and then
sequenced with a \mRNA\ paired-end protocol on Illumina HiSeq 2000/2500
sequencers which produced an average of 80 million reads.

For privacy reasons, the raw data is available only through controlled access via
\dbGaP{phs000424.v4.p1} (access number specific to the version of the data I used
in my study). While getting access can take time, in principle every request for
academic research should be granted.

\section{Mass spectrometry Proteome data}

While the transcriptomic data was retrieved and then processed within the \EBI\
either by Dr Nuno Fonseca or myself, the proteomic data have been picked and
handled by Dr Jyoti Choudary and Dr James Wright from the Wellcome Trust
Sanger Institute.

Until a couple of years ago, compared to the transcriptome, the proteome world
was regrettably lacking on normal human tissues expression quantification
experiments. In fact, while there were human protein maps available
(\eg\ the \href{www.proteinatlas}{Human Protein Atlas}%
\footnote{Human Protein Atlas --- \href{http://www.proteinatlas.org}%
{www.proteinatlas.org}}), these
are mostly reporting the spatial expression of proteins (as they are based
on immunohistochemistry or other means of identification) than quantifying
their (non-targeted) abundance in each tissue.

Then, in 2014, two different groups of authors, M.-S.\ Kim et al.\
and Wilhelm et al., published (in \textit{Nature},
issue 7502) their own \emph{``draft of the human proteome''}
based on the study of tissues with \ms. These two datasets complement a previous
smaller one that was publicly released but was never the object of a publication.

Hereafter, I present these 3 datasets that I use in my thesis.

\subsection{Pandey Lab data}

The Pandey Lab \mycite{PandeyData} created the
\href{http://www.humanproteomemap.org/}%
{Human Proteome Map}\footnote{Human Proteome Map --- %
\href{http://www.humanproteomemap.org/}{www.humanproteomemap.org}} which
they released along \paper{\citetitle{PandeyData}} in \emph{Nature}.

For their study, they have processed 30 kinds of histological normal human
tissues and cell line samples (17 adult tissues, 7 fetal tissues and 6
haematopoietic cell types). The samples were created from pooled samples of three
individuals (generally two males and one female).

Then, they prepared their libraries with a label-free method to quantify
as many proteins they could. They fractionated the samples to protein level by
\gls{SDS-PAGE} and then at peptide level by \gls{RPLC} to create 85 experimental
samples. Finally, they use state-of-art \gls{MS/MS} protocols
(with high-resolution and high accuracy \glspl{FTMS}:
Thermo Scientific Orbitrap instruments).
They generated about 25 million of (\gls{HCD})
high-resolution mass spectra which account for 2,212 \gls{LC-MS/MS} profiles.

The raw spectra were retrieved from ProteomeXchange via the repository
\Pride{PXD000561}.

While their effort to generate technical high quality raw data was highly
appraised by the scientific community, their processing
(identification and quantification) methods were
criticised (see~\cite{Ezkurdia2014-qx}). Thus, for this thesis I only use
quantification that James Wright provided me after reprocessing the dataset
from the raw spectra.

\subsection{Kuster data}

As for the Kuster Lab approach of the human proteome map,
\cite{KusterData} combined newly generated \gls{LC-MS/MS} spectrum
data (about 40\%) with already published one
(either from their colleagues or accessible through repositories ---
for the remaining 60\%).
The data comprised 16,857 experiments involving tissues, body fluids and cell
lines. They used all the data they could access from \gls{PTM} to affinity
purification studies.

They reprocessed the whole collection of spectra to maximise proteome coverage
and make it available through their own repositories: ProteomicsDB\footnote{%
Proteomicsdb --- %
\href{https://www.proteomicsdb.org/}{www.proteomicsdb.org}}.

The subset of data considered in my thesis is also
known as the [protein] Human BodyMap which is the part that was primary generated
by the Kuster lab itself for their study. It corresponds to 1,087 \gls{LC-MS/MS}
profiles and comprises 48 experiments covering 36 tissues (adult and fetal) and
cell lines. Overall that represents about 14 million of \gls{HCD}/\gls{CID}
spectra from Thermo Scientific instruments.

This specific subpart of the raw data was downloaded from \Proteomicsdb{PRDB000042}.

\subsection{Cutler data}
\begin{comment}
    rexpliquer quelle partie reutilisée
\end{comment}

This data was generated prior to the \dataset{Pandey} and the \dataset{Kuster}
data as it was released in 2011 through PeptideAtlas\footnote{PeptideAtlas --- %
\href{http://www.peptideatlas.org/}{www.peptideatlas.org}}
\mycite{PeptideAtlas}.

It was created by Paul Cutler at Roche Pharmaceuticals.
It comprises 10 different tissues (1,618 \gls{CID} Thermo Scientific raw files).

While this data was never published on its own, it has been used in different
studies. %Indeed, the \dataset{Cutler} data is one of the datasets that
%\cite{KusterData} are using in their original study to create ProteomicsDB.

The raw files were accessed and downloaded from \Proteomicsdb{PRDB000012}.


\section{Constant processing pipelines}

The original authors of these 5 transcriptomic and 3 proteomic studies have often
released themselves the quantification of the expression values either directly
(\eg\ \cite{Krupp2012}) or upon requests (\eg\ \cite{PandeyData}).
Third-parties also distribute
quantification for these studies either retrieved from the original studies,
as BioGPS \mycite{BioGPS1} or Harmonizome \mycite{Harmonizome},
or after reprocessing the raw data as EBI Gene Expression Atlas
\mycite{EBIgxa} does.

For many reasons and despite readily available quantifications for most of the
datasets, I only used data reprocessed from raw files by myself or Dr Nuno
Fonseca  (Gtex data) or Dr James Wright (proteomic data).

In fact, each study has been originally processed with different protocols,
\eg\ \dataset{\Gtex} data \mycite{GTExTranscript} and \dataset{Castle}
data \mycite{Krupp2012}.  In addition, while the \egxa\ reprocesses raw data
through the same methods
and has nowadays quantification for most of the datasets, these are still the
products of different protocols since
different versions of reference (Human genome build and annotation) and tools
have been used.

Intuitively, we expect that different processing protocols produce
different results.

% est-ce que tous ces concepts sont introduit précedement.
Indeed, as I started to work with \Rnaseq\ data, I noticed many potential bias
sources that impact its final expression values. Many of these have been
reported in the literature since then;
annotation versions \mycite{annotationDiff},
contamination \mycite{contaminationRNAseq},
quality controls \mycite{qualityRNAseq},
mapping and quantifications pipelines \mycite{Fonseca2014}
have great effects on the final quantification. Lastly, normalisation
methods also greatly impact the final expression figures
\mycite{Dillies2013}, \mycite{normalisation2}. For all these reasons, reprocessing
all the transcriptomic datasets --- with the same exact protocol ---
was the logical first step of this current study.

Likewise, there are various tools and many parameters for each processing step
needed to quantify the proteomic data. Therefore, Dr James Wright reprocessed
uniformly the 3 datasets from the raw spectra till the normalisation of the
protein expression values.

\subsection{Transcriptome RNA-Seq raw data processing}

There are many steps from the raw data files to the quantification matrices
on which this thesis analyses are based. \Cref{fig:pipelineTrans} presents a
general overview of the \Rnaseq\ processing protocol I used.

\begin{figure}
    \includegraphics[scale=0.75]{additional/pipelineTrans.pdf}\centering
    \caption[General steps for processing the transcriptomic
    data]{\label{fig:pipelineTrans}\textbf{General steps for processing the
    transcriptome} [TK]}
\end{figure}

I downloaded and entirely processed four of the transcriptomic datasets
myself (\dataset{Castle}, \dataset{Brawand}, \dataset{IBM} and \dataset{Uhlén}
data) and Dr Nuno Fonseca retrieved and processed the \Gtex\ dataset (\hg{38.p1}
human genome reference and \ens{76} annotation)\footnote{As
the \Gtex\ data is involved in many project within the \EBI\
and due to its huge amount of files (number and  size -- see \Cref{tab:Lib5DF}),
it was agreed that this would be processed centrally by one person and then
redistributed to all the other interested parties. Dr Nuno Fonseca had this
tremendous task.}. I present results computed on the quantification of these
5 datasets which have been processed through the same identical pipeline.


\begin{table}
\centering
\caption[Technical description of the 5 transcriptomic datasets]{%
\label{tab:Lib5DF}\textbf{Technical description of the 5 transcriptomic
    datasets}\\\footnotesize{I processed all the datasets but the one in
    \textit{\color{darkgray}italic}.\\
    For the Brawand dataset, I only processed the \species{Homo sapiens} part.}}
\begin{tabular}{@{}cccccc@{}}
\toprule
Dataset & \begin{tabular}[c]{@{}c@{}}Participant\\number\end{tabular} &
\begin{tabular}[c]{@{}c@{}}Library\\number\end{tabular} &
\begin{tabular}[c]{@{}c@{}}File\\number\end{tabular} &
\begin{tabular}[c]{@{}c@{}}Total size of\\the fastq\\raw files (GB) \end{tabular}&
    \begin{tabular}[c]{@{}c@{}}Mean number of\\biologic samples per\\tissues
    [min,max]\end{tabular} \\
\midrule
Castle        & 10    & 11    & 11    & 58    & 10 (mixture)    \\
Brawand       & 18    & 21    & 23    & 111   & 2.8  [2,3]      \\
{\small Illumina Body Map} & 16    & 36    & 48    & 1,004  & 1 \\
Uhlén         & 122   & 200   & 400   & 1,851  & 3.81    [2,11] \\
    \textit{\color{darkgray}\Gtex\ (v4)}  &
    \textit{\color{darkgray}551}  & \textit{\color{darkgray}3,276}  &
    \textit{\color{darkgray}6,552}  &
    \textit{\color{darkgray}$\sim$ 50,000}  & \textit{\color{darkgray}60.67 [4,214]}\\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Data retrieval and preparation}

I retrieved the human raw data of each dataset from \gls{ArrayExpress} and
\gls{ENA} through their identifier (see \cref{sec:rnaseq-data}). After we
received our access approval, Dr Nuno Fonseca retrieved \Gtex\ data from
\gls{dbGaP}.

While most of the raw files can be used as they are, an additional step is
needed for the \dataset{Castle} files. Indeed these files are using an older
\fastq\ format that is non-compliant to the most accurate and recent tools.
As it is a simple matter of changing the quality score scale,
I translated these files to \gls{Phred} $33$ \fastq\ files with a
\emph{\gls{Perl}} script (see~\Cref{code:fastq}).

\subsubsection{Genome and annotation reference}

I accessed the datasets through an extended period of time. Hence, for a subset of
them, I produced many sets of results based on the \hg{37.p12}
(and later \hg{37.p13})  human reference genome and the latest available ENSEMBL
annotation (73, 74 or 75) at that time.
In fact, the quality of each new annotation update is
supposedly greater than its predecessor.

As the \dataset{Gtex} data was processed with \hg{38.p1} and \ens{76}, that led me
to reprocess all the other four \Rnaseq\ datasets for the sake of consistency and
to avoid more biases \mycite{h38vsh37}. Thus, unless indicated
otherwise, the results presented in the current work are
based on the \hg{38.p1} human genome reference genebuild and the \ens{76}
annotation.


\subsubsection{Data processing}

In the early stages of my research, I was processing each of the different steps
sequentially and semi-manually with the help of custom made scripts. While the
\EBI\ \gls{cluster} greatly facilitated the handling of the numerous files,
the task was quite tedious: particularly due to the necessary jobs monitoring of
each step and the integrity checks of the intermediate files between them.
Additionally, although working perfectly within the \EBI\ infrastructure,
the scripts I wrote would need a fair amount of work to achieve reproducibility
for other platforms.

Fortunately, Dr Nuno Fonseca developed and released
an ``integrated RNA-seq analysis Pipeline'': \irap\footnote{\irap\ ---
see: \href{https://nunofonseca.github.io/irap/}%
{https://nunofonseca.github.io/irap/}}.
This tool allows the automation of the typical
state-of-the art workflow to study
\Rnaseq\ and use advanced software optimisation techniques and takes full
advantages of the capacities provided by computer clusters. Hence, my previous
manual processing workflow get optimised by \irap\ without any real change
regarding the steps or the algorithms. % à relire phrase.

Besides the usual inputs files (raw \Rnaseq\ files and genome/annotation
references), \irap\ needs a configuration file that precisely describes the
dataset (its design and technical features) and, if needed, specific parameters
to use. Indeed, \irap\ provides many default options that could be easily
overridden by explicit definitions in the configuration file. To provide
full reproducibility, each version of \irap\ is shipped with
its own set of third-party version-defined tools and default parameters. Thus,
apart of remarkably speeding up the data processing, \irap\
also ensures the protocol integrity across the 5 transcriptomic datasets I
use in my thesis regardless of who run the pipeline.

Each of the transcriptomic datasets are the products of the same version of
the \irap\ pipeline (development version 0.6.3b) and set of parameters.

As the default parameters of \irap\ are tuned for human Illumina paired-end data,
I only have to define few of them. Hence, the quality and contamination checks,
the filtering and trimming of the reads are done following the default options
of \irap.


\minisec{Quality assessment, trimming and filtering}

\irap\ uses internally \soft{FastX} toolkit\footnote{\soft{FastX} toolkit ---
\href{http://hannonlab.cshl.edu/fastx\_toolkit/}%
{http://hannonlab.cshl.edu/fastx\_toolkit/}} (0.0.13) to perform the
assessment and the trimming.

Reads with a low sequence complexity (\eg\ poly-T or poly-A tails) are discarded.
As is any read with uncalled bases (N) or with an overall quality score below a
threshold of 10.

As reported in \Cref{ch:Introduction}, as the base calling progresses, quality
of the call decreases. On another note, some tools (mappers in particular) need
all the reads to be trimmed to the same length. \TK{\irap\ will optimise the
purity-length balance to avoid more errors or biases due to smaller reads
\mycite{Trimwisely} by trimming at most 15\% of the original length while discarding
more reads if necessary to maximise the length. : instead balance compromise}

Reads that could be assigned to a likely contamination source (here
\species{Escherichia coli}) are also discarded. A splice non-aware mapper,
\soft{BWA}\footnote{\soft{BWA} --- \href{http://bio-bwa.sourceforge.net/}%
{http://bio-bwa.sourceforge.net/}} (0.7.4) maps all the reads to the contaminant
genome and all the reads mapping perfectly and non ambiguously are discarded.

\minisec{Mapping}
I mapped the reads to the genome with \irap's proposed default splice-aware mapper
\soft{TopHat2}\footnote{\soft{TopHat} ---
\href{https://ccb.jhu.edu/software/tophat/index.shtml}%
{https://ccb.jhu.edu/software/tophat/index.shtml}} (2.0.12) \mycite{tophat2}
with its set of default predefined arguments. Hence, I only had to provide the
genome and the transcriptome references. Indeed, \soft{TopHat2} can handle reads
from many organisms by fine-tuning the parameters (\eg\ number of mismatches or
indels to tolerate) although the default parameters are adjusted for normal human
transcriptome reads.

\soft{TopHat2} uses an hybrid approach between the two main
strategies to reconstruct a transcriptome, that are either by mapping them to
a reference or by \latin{de novo} assembly (see \cref{ch:Introduction}).
As a first step, \soft{TopHat2} tries to align all the reads
to the genome. Then in a second time, it maps all the unmapped reads from
the previous step to the transcriptome reference and creates an exon-exon
junctions database based on the new set of mapped reads. Finally, all the
remaining unmapped reads are split between possible exons as to find
possible new split events via a seed-and-extend approach.

\minisec{Quantification and Normalisation}

While \Rnaseq\ is a technique of choice to identify (and discover) \gls{RNA}
isoforms, I have focused my thesis on the gene level expression. Indeed, current
annotations and knowledge are still lacking in the reasons and
external conditions that impact the expression of a specific isoform over the
others. In addition, criticisms have been raised on the accuracy of distinction
between them (\cite{tamaraRNA}, \cite{ernestRNA}). While normalising gene
expression presents additional challenges than specific transcript expression,


As I discuss in \Cref{ch:Introduction}, there are various strategies to estimate
gene expression levels. I used two approaches, implemented in these
following popular tools:
\soft{Cufflinks}\footnote{\soft{Cufflinks} ---
\href{http://cole-trapnell-lab.github.io/cufflinks/manual/}%
{http://cole-trapnell-lab.github.io/cufflinks/manual/}} (2.2.1) \mycite{cufflinks}
and \soft{HTSeq-count}\footnote{\soft{HTSeq-count} ---
\href{http://www-huber.embl.de/HTSeq/doc/index.html\#}%
{http://www-huber.embl.de/HTSeq/doc/index.html\#}} (0.6.1p1) (with the
\texttt{intersection non-empty} mode).


\soft{Cufflinks} tries to probabilistically assign the multi-mapped reads
depending on the coverage at each mapping locus. It also

\soft{HTSeq-count}

\gls{FPKM}
values were then calculated with the internal function provided by \irap.
I am using only gene levels quantification and not isoforms.


\subsubsection{Tissue Averaging}

To avoid unnecessary skewness due to the unbalance of biological replicates per
tissue across the set of datasets for many of the analyses, \TK{reexpliquer pkoi
le déséquilibre}
I computed a \emph{virtual reference tissue}
for \dataset{Brawand}, \dataset{Uhlén} and \dataset{Gtex} datasets.

For each of these datasets, for each tissue, I compute gene expression levels by
taking the median value of each gene across all the biological replicates.

The \dataset{Uhlén} requires an extra prior step for some of the tissues as they
present technical replicates. For these, I average first the gene expression levels
for each subject-tissue pairs before computing the gene expression level medians
of each tissue.

\NB \dataset{Castle} dataset comprises mixture of 10 subjects for each tissue, I
discard the single-end sequenced samples for the \dataset{Illumina Body Map}.


\TK{Isabelle, c'est la fin de ce que j'ai fait pour le moment}



\subsection{Proteome raw data}

Similarly to the transcriptome issue, there is the proteome which is actually
trickier (possible problem with \dataset{Pandey} data analyses). Really depends which
threshold and \gls{FDR} tolerate. And what database and algorithm are used to perform
the identification of the proteins. More messy.

All the data has been processed by James.

Small problem about mapping back proteins to transcripts.



And again describe the methodology.

  \begin{figure}
      \includegraphics[scale=0.75]{additional/pipelineProteins.pdf}\centering
      \caption[General steps for processing the proteome
      data]{\label{fig:pipelineProt}\textbf{General steps for processing the
      proteome} [TK] }
  \end{figure}

\section{Discussion/Conclusion}

In this chapter, I describe why and how 5 transcriptomic and 3 proteomic normal
human tissues datasets have been reprocessed from raw files with unified
pipelines using the same reference and annotation files.

As I accessed the datasets through an extended period of time, for a subset of the datasets, I produced many sets of results with the previous human reference gen- ome (GRCh37) and three different ENSEMBL annotations (73, 74 and 75).While the results can vary for individual genes, the overall outcomes are congruent together hence supporting the robustness of the findings presented in this thesis.

\TK{Many tools are for now not a better than the other one. Can be hard to decide
which one to pick. A lot of decision have to been taken --- and while this
doesn't affect the overall landscape, if one is interested by a specific
genes, \Rnaseq\ should be supported with more wet-lab thing.\\
    Add shortly other important information (or maybe mix in a bit of discussion.
Too vague now) to the conclusion \\
All but one of these datasets have been subject of one if not more papers:
results confirmed previous authors.\\
Similar expression between HTSeq and Cufflinks
\\
Add the end of the day, \Rnaseq remains an exploratory (even when we look at
specific differential expression) studies.
%Discuter de l'origine des échantillons
}


