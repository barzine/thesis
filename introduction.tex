\chapter{Introduction}
\label{ch:Introduction}

\begin{comment}
\setlength{\epigraphwidth}{0.6\textwidth}
\setlength{\epigraphrule}{0.1pt}
%\epigraphhead[70]{%
    \epigraph{It would probably be oversimplifying the
matter,\\but I am strongly tempted to say,\\ ``All life is nucleic acid; the rest is
commentary''.}{\cite{asimov:WrongRel}}%
%}
\end{comment}


\begin{comment}
When Asimov concluded with these words his chapter ``Beginning with bone'',
scientists had already started unfolding one of the most mesmerising biological
mystery: how does the Nature manage and organise the production of specific
effectors in specific locations. In other words how from one cell
\TK{revoir cette phrase: idea initiale pourquoi
organisme A different
de B, ou un foie et un foie et pas un coeur + comment d'une cellule-oeuf on a un
organisme complet qui se creée... ne pas s'étendre dessus par contre}.\
Watson and Crick by publishing the double-helix structure of \DNA\
\mycite{DNA1953} unlocked our path to understand the natural ways of storing and
manipulating the information. As the whole past six decades were packed with major
discoveries and technological achievements, there are many, fascinating,
milestones that could be discussed.
The following pages present a summary of the facts and techniques
that form the biological and technical context of the work backing this thesis.

--- ici intro par rapport à Asimov et le fait que l'ADN est un 'blueprint' et qu'en
fonction du l'organe ou du develepment stage l'expression est régulé. On sait
maitenant que la régulation se passe à différent niveau.... finir peut être la
section sur Harvey qui a élucider la circulation sanguine car il a utilisé
l'observation et quantification du sang ---


As where the situation stands for now, we know that between the unique common
bluprint, which is the \DNA\ and the different phenotypes displayed by the tissues
comprising our bodies, they are many regulatory processes happening at different
layers (transcription, and translation).

 ### agent


\Rough{%
Gene expression, transcription, translation, regulation at each stage,
expected and reported correlation between transcripts and proteins
Measuring transcript expression by RNAseq, wet-lab part, analysis methods,
available large scale datasets on human tissues
Measuring protein expression by MS, data analysis – spectral counts, top 3, etc,
available large scale datasets on human tissues
The problem of comparing and integrating independent datasets, EBI’s GXA \\ \\
key concepts: DNA storage, RNA transfert of information and regulation,
protein effectors ==\textgreater phenotype.\\ \\
%
Do not forget the (tissues) specifications and the structural functions. =\textgreater Embryology.\\ \\
%
introduction real possible start: la soupe primaire -\textgreater creation de RNA, aa,
oeuf/poule
premiere cellule, \ldots (lire Darwin, il y a ptet moyen d ajouter une citation ou
quelque chose.%
}

\begin{itemize}
    \item Proteomics messy and noisy, challenging \ldots
    \item Transcriptomics seem a good proxy to study the phenotype => one kind of molecule made of the same pieces.
    \item Technology improved a lot from the microarrays to sequencing.
\end{itemize}

\end{comment}


\section{Diversity and universality of Life}



%\TK{Soupe primaire experience du Chimiste}
\TK{The central dogma of Molecular biology} => Crick describes the cycle

\fixme{Add references on replications, not going into that one}

\subsection{Translation + Regulation}

\subsection{Translation + Regulation}

Just so that the next part of the chapter got a OK presentation:
\begin{itemize}
    \item \gls{ncRNA}
    \item \gls{tRNA}
    \item \gls{miRNA}
    \item \gls{snRNA}
    \item \gls{scaRNA}
    \item \gls{snoRNA}
    \item \gls{scRNA}
\end{itemize}

\TK{Assumption mRNA and proteins : highly correlated [find references]}


\section{Capturing the expression in the laboratory}

Biological research uses mainly two approaches to
study the cell life intricacy and its underlying mechanisms.\\
The oldest approach is descriptive:


Small bits: corrections for Rnaseq are already part of the analysis and
requires often as much flair than skills.
Contrarily to \Dnaseq\ where corrections can be applied \TK{add reference} and
then the analysis be done, in Rnaseq each analysis requires a set of conform
quantification and normalisation methods.
While, there are quite established protocols for differential expression analysis,
there are presently many other downstream analyses that are cumbersome
and/or not settled yet. This is the case for this study.


The transcriptome is the total repertoire of transcripts (\ie\ \glspl{RNA}
molecules) expressed in a cell or tissue at a given time and condition. Unlike
the genome which is roughly identical regardless which cell of a particular
individual is considered, the transcriptome varies\ldots

\subsection{Transcriptome exploration with RNA sequencing}

While microarrays are measuring many \mRNAs\ at once, their number is limited,

\TK{history of \Rnaseq.}
In the past decade, \gls{RNA-Seq} technology has risen as the method of choice
for  transcriptome.

Many methods and technologies through the years but more recently, boom of study:
next generation sequencing (1st generation, 2nd generation and 3rd)\ldots So
much that now the expression doesn't mean anything.

Completion of the Human genome project : key changer: probes with microarrays
possible (as there were then template). Next key changer: shotgun sequencing
instead of Sanger sequencing (slow). + advance in computer science: needs of
parallelisation and storage.

So, In 2008, shift from microarrays to \Rnaseq.

\clearpage

In the following section, I introduce the typical steps of the required workflow
to study the transcriptome through sequencing on an Illumina platform. In fact,
while not by conscious design, all the transcriptomes analysed in this thesis are
the product of Illumina sequencing (see
\crefrange{subsec:castlepresentation}{subsec:gtexPresentation}).
It is not surprising as Illumina was by far
the most popular platform for the last decade \mycite{popularIllumina}.
Indeed, Illumina proposes a very
good ratio between the accuracy and the fact that it achieves the highest
throughput and the lowest per-base cost \mycite{IlluminaCheap}.

Experimental protocols for other platforms will need various and specific
modifications that are outside of the realm of this thesis and thus will not be
cover here. More details on the other main sequencing platforms and their
relevant protocols can be found in \cite{rnaseqProtocols} review paper or at the
online resource
``\href{http://rnaseq.uoregon.edu/}{RNA-seqlopedia}''\footnote{\textit{RNA-seqlopedia}:
\href{http://rnaseq.uoregon.edu/}{http://rnaseq.uoregon.edu/}.}
\mycite{rnaseqlopedia}.

As I discuss the concepts behind the Illumina sequencing technology and the
most common related methods to process it, I will emphasise the approaches and
the tools I used to estimate the gene expression levels from raw nucleotide
sequences.

\Cref{fig:OverviewRnaseqPrepSeq} presents an overview of the typical steps of a
\Rnaseq\ workflow from the libraries preparation to the sequencing.

\begin{figure}
    \includegraphics[scale=0.60]{introduction/IntroWorkflow.png}\centering
    \caption[Overview of a \Rnaseq\ workflow: library preparation
    and sequencing]{\label{fig:OverviewRnaseqPrepSeq}\textbf{Overview of
    a typical \Rnaseq\ workflow:
    library preparation and sequencing}}
\end{figure}

\NB Albeit the collection and the conservation of the samples prior to the
\gls{RNA} extraction most definitely affects the final estimations,
I will let aside these steps from my review.

\subsubsection{Library preparation}

While there are sequencing technologies that can directly sequence \glspl{RNA}
(see \mycite{rnaDirectSeq}), most of the technologies handle only \gls{DNA}.
Hence, the first step of a typical \Rnaseq\ workflow is the preparation of
\gls{cDNA} libraries from the starting material. This step and the sequencing
itself are the most platform dependent parts of the overall protocol.
Indeed, contingently on to the sequencing principle they rely,
the sequencers need the libraries to be fixed and loaded differently.

\minisec{\gls{RNA} extraction}
There are many methods to extract \glspl{RNA} from the primary samples and they
are commonly standardised. Indeed, regarding the type of biological samples,
the \glspl{RNA} of interest, the aim of the  study and the sequencing platform
used, there is one (or more) available commercial kit. These are designed in
a way to not interfere
with any of the later steps of the library preparation or with the sequencing
itself.

Without surprise, the choice of one kit (and hereby method of extraction)
over another can impact the final \Rnaseq\ data. The main difference between
the most widespread methods being the quantity of non-mature \glspl{RNA}
(\ie\ with longer intronic regions) detected whereby kit has been used.
However, the relative gene expression levels are similar from one extraction
protocol to the other. \mycite{RNAextraction}


\minisec{\gls{RNA} enrichment}

After extracting the \gls{RNA} from the cells or tissues,
the next step is to enrich the content of the samples with the \glspl{RNA}
of interest (\ie\ the concentration of the \glspl{RNA} of interest is increased
either by specifically selecting it or by removing other \glspl{RNA}). Indeed,
the \glspl{rRNA} are the most abundant type of \gls{RNA} in any cell. Even
though they amount for a very small part of the genome\footnote{For example,
\species{Homo sapiens}, there are 568 genes (<1\%) that are described as
\gls{rRNA} out of the 63,898 annotated genes of the ENSEMBL database
(\hg{38.p10} - \ens{89}).}, they represent by their number 70\% or more of
the total population of \gls{RNA} \mycite{biochbook}.
Although there are interests to study \glspl{rRNA} (\eg\ \mycite{rrnaStudy}),
\mRNAs\ studies are more
popular and they only constitute about 3 to 5\% of the whole \gls{RNA} population
\mycite{molBiolCell}. Other studies research even scarcer kinds of
\gls{RNA}.\footnote{Out of the 10,081 experiments tagged as \comp{``rna assay''}
and \comp{``sequencing assay''} within \gls{ArrayExpress}, 7,981 were also
tagged as \comp{``RNA-seq of coding RNA''}, 1,829 as \comp{``RNA-seq of
non coding RNA''} and 366 have both tags. 4 of them are only described as
\comp{``microRNA profiling by high-throughput sequencing''} --- Query date: 22
June 2017.}

There are typically three strategies to achieve \gls{RNA} enrichment:
either by polyA-selection, by ribodepletion or (more complex)
by targeted amplification. While these
approaches are insufficiently specific to select one particular kind of \gls{RNA}
or remove all \glspl{rRNA}, it eases and improves the downstream analyses.

\subminisec{PolyA-selection}
This strategy essentially targets the \mRNAs. It exploits the polyadenylated
tail at the 3' end of the \mRNAs\footnote{And a few other kinds of \gls{RNA},
\eg\ \glspl{lncRNA} \mycite{polyAlncRNA}} that is added
post-transcriptionally. In fact, magnetic beads supporting short
strings of thymine (oligo-dT) capture efficiently these \mRNAs\ while the others
are washed away \mycite{Mortazavi2008}.

This protocol is probably the most widespread one as it is the easiest and
cheapest to set up. A dataset produced following this protocol is known as
a \emph{polyA-selected} dataset.

\subminisec{Ribodepletion}
This strategy is preferred for the study of any \gls{ncRNA} or when researching
the interaction of \mRNAs\ with other \glspl{RNA} \mycite{ribodepletion}. This
strategy is in a way the reverse of the previous one as its also
uses magnetic beads, but this time to efficiently\footnote{ThermoFisher claims
that its RiboMinus protocol can remove till 99.99\% of the \glspl{rRNA}.}
target the unwanted \glspl{rRNA} as to remove from the sample.

The ribodepletion can also be achieved through ribonucleases. These enzymes
specifically digest \glspl{rRNA} and then \glspl{RNA} of interest can be retrieve
through size selection.

Datasets produced following a ribodepletion protocol are usually called
\emph{whole \gls{RNA}} or \emph{total \gls{RNA}} as a contrast to the
\emph{polyA-selected} ones.

\cite{castleData} created a total \gls{RNA} dataset but they use another
approach where they amplify \emph{every} other \gls{RNA} with the help of
specifically designed probes (see \cref{subsec:castlepresentation}). Hence,
the protocol they have used is closer to the following one.

\subminisec{Targeted amplification}
Targeted amplifications rely on primers that would be designed to target (or
avoid as for \cite{castleData}) specific sequence motifs of the genome. Most
studies based on this kind of approach are not referred as \Rnaseq\ studies, but
a name that is based on the studied \gls{RNA} type (\eg\ \gls{miRNA-Seq}) or
that emphasises the variation of the method (\eg\ Capture-Seq
\mycite{captureSeq}). Often, in comparison with a polyA-selected or ribodepleted
dataset, additional steps are required to prepare the libraries.


\minisec{RNA fragmentation}
Most of the sequencing platforms\footnote{As for the Illumina platforms that have
produced the transcriptomic datasets studied in this thesis.} require relatively
short (\ie\ 200 to 500 nt) length to sequence. Concomitantly, it also ensures
that the sampling along the \gls{RNA} is more uniform.
This fragmentation can be carry out via divalent cations hydrolysis or
nebullisation.

This step is performed on occasions after the \gls{cDNA} synthesis
(see next section). In those case, the \gls{cDNA} are fragmented mostly
by digestion with DNase I or by sonication.

\minisec{Double-stranded cDNA synthesis}
The \gls{RNA} molecules are used as a template for a retro-transcription
involving oligo-dTs or \emph{random} hexamer primers, respectively only
for polyA-selected datasets or any dataset (polyA-selected included).
The set of \emph{random} hexamer has been designed to cover the whole
transcriptome. Unfortunately, these \emph{random} hexamer primers have been
proven to actually lack full randomness \mycite{notSoRandom}.

At the end of the most common protocol, the order of synthesis of each \gls{cDNA}
strands is lost, \ie\ it is impossible to distinguish which of the \gls{cDNA}
strands has the same sequence than the original \gls{RNA}. Several techniques,
called \emph{strand-specific}, have been developed to compensate for this
(\mycite{strandSpecific}, \mycite{strandSpe}).

\minisec{Adapter ligation, PCR amplification and size selection}
After generating blunt edges by restriction digest of the \glspl{cDNA}, adapters
(small known sequences of oligonucleotides) are ligated to their both ends.
These adapters are constituted from several parts. A subset of them are ensuring
later the hybridisation of the \glspl{cDNA} with the flow
cells\footnote{\gls{flow}: see \cref{subsub:HybridClustAmp}.} (based on sequence
complimentary) and another set of them
are sequence binding sites that are used as primers for the following cluster
amplification step happening \latin{in situ}. These adapters are also used to
introduce additional motifs such as indexes.

The next two steps can be interchanged regarding the amount of starting material
at disposition. All the molecules are amplified by \gls{PCR} before (or after)
a size-selection is performed (per gel electrophoresis) to extract
length-complying fragments (about 200 to 500 bp) to the sequencer machine
requirements\footnote{Indeed, the previous fragmentation step creates a great
length range of fragments.}.

Unfortunately, the size-selection means that any
transcript with an original length below the threshold used for the
selection will be missed\footnote{There is no problem for the greater length
as statistically they will present fragments that will be in the correct range.}.
For example, \glspl{miRNA} are shorter than the general requirement of Illumina
sequencers. Alternative protocols are addressing this issue
\mycite{smallRNAprotocol}.

\minisec{An example of alternative preparation strategy}
Along with the targeted, the strand-specific and small \glspl{RNA} protocols,
there are a few other variations to this typical protocol to handle other concerns.
For example, it is occasionally necessary to sequence simultaneously (in a single
run) multiple samples. This can be motivated by practical reasons
(to lower the experimental costs or hasten the overall processing time)
\mycite{multiplexCheaper} or critical to the experimental design as a way to
experimentally handle the \emph{batch effects}\footnote{Batch effect: see
\cref{subsub:BatchEffect}} \mycite{multiplexBatchEffect}. However, it is
crucial to later extricate the several pooled samples from each other as a
requirement to many downstream analyses.

\emph{Multiplexed} protocols easily achieve
the distinction between the multiple samples as they incorporate \emph{barcodes}
before ligating the adapters. These barcodes are also small sequences of
nucleotides and each sample has its own unique associated
barcode. In practice, each sample is prepared separately with the added extra
step (before the adapters ligation) where the barcode is incorporated, then all
the samples are pooled together before the next step that consists to hybridise
the \glspl{cDNA} to the flow cell.

Other extra steps occur just after the sequencing and before any other data
analysis: all the reads\footnote{Reads : see \cref{subsub:sequencing}} are
separated in files based on their barcodes and the barcode, along with the
adapters, is trimmed from all the reads.

The main inconvenient of the multiplexing protocol is that the original sequenced
length of the \glspl{cDNA} are then shorter as the barcodes are also (and have
to) be sequenced as well.

\subsubsection[Clustering: Hybridisation and Bridge amplification]{Clustering:
Hybridisation and Bridge amplification \mycite{IlluminaYoutube}}
\label{subsub:HybridClustAmp}

Once the libraries are ready, they are loaded onto a \emph{flow cell}\footnote{%
\emph{Flow cells} are the support of Illumina sequencing. They enable the
parallelisation of the sequencing of millions of \gls{DNA} fragments together
which are kept spatially separated in clusters. This was allowed with the advent
and optimisation of supported Chemistry. Each flow cell is a glass slide with
lanes. Each lane is coated with two short nucleotide sequences. One of these
oligonucleotides is complementary to a region comprised in the ligated adapters.}.

The clustering step comprises two phases: hybridisation and bridge amplification
of the \gls{cDNA} fragments.

\minisec{Hybridisation}

The double-strand \glspl{cDNA} are denatured and then each fragment randomly
hybridises across the flow cell surface with one of its small oligonucleotides.
These are used as primers for polymerases which create a first complimentary
strand to the hybridised \gls{DNA} fragments. The new double-strand molecule is
denatured and the original first template is washed away.

\minisec{Bridge amplification}
The strands then folds over and their (second) adapter hybridises with a
complimentary oligonucleotide sequence of the flow cell and thus creating a
bridge. The flow cell complimentary fragment is then used as the primer for a new
strand. The new double-stranded \gls{DNA} is then denatured (which
dismantles the bridge). Each of the two tethered molecules creates a new
bridge by hybridisation which are the templates for a new strand each.
This process happens many times and simultaneously for millions of fragments.
It creates clusters of clonal amplification of the original fragments of
the library. After the bridge amplification, the reverse strands are cleaved
and washed away. The 3' end primers are also blocked to avoid any unwanted
priming.

\subsubsection{Sequencing-by-synthesis}
\label{subsub:sequencing}

Illumina sequencers propose two sequencing approaches: single-end and paired-end.
The difference is that in \emph{single-end sequencing}\footnote{Chronologically
the oldest method}, the sequencing begins at one (and only one) of the fragment
ends and progresses towards the second. Whereas in \emph{paired-end sequencing},
once the first end has been sequenced (as in the single-end approach), after a
single new bridge replication, the other end of the original fragment is
sequenced as well. Hence, in paired-end, the sequencing occurs at \emph{both}
ends of each original fragment. This has been allowed by a small change in
the original single-end adpters \mycite{pairedEndAdapters}.

Though more expensive and more challenging to programmatically handle,
the paired-end approach produces more information and hence facilitates
the detection of genomic rearrangements (as indels or inversions) and
repetitive sequence elements.
It also allows an easier distinction between isoforms of a same gene and provides
greater support to the novel transcripts (new isoform or new gene) and fusion
genes\footnote{A \gls{fusionG} is a gene that is the product of the fusion of
parts of two different genes.}.

In both cases, the sequencing process is the same. It is an Illumina proprietary
process and it is called \emph{Sequencing-by-Synthesis} \mycite{seqBySynth}.
It uses the \gls{DNA} replication mechanism with modified \dNTPs.
It relies on the step-by-step incorporation of
reversible fluorescent tagged \dNTPs\ who are protected at
their 3'end to block any further elongation.
The product of this synthesis is called a \emph{read} and it supports the base
calling.

The sequencing occurs simultaneously on every identical fragment
of every cluster of the flow cell.
It begins with the hybridisation of a complimentary 5' primer onto the 3' biding
site of the tethered \gls{DNA} template. This primer is then extended by
replication to create a new read. Next, the sequencing cycle (described below)
repeats itself several times.

The \emph{sequencing cycle} starts with the addition of one complimentary
fluorescent \dNTP\ to the new growing read and, since the \dNTPs\ are blocked at
their 3' end, the replication process stops. Afterwards, all the unlinked
\dNTPs\ are washed away. Then, the clusters are excited by a light source
and as each
\dNTPs\ has a fluorescent tag with its own characteristic wave-length, the
signal that they emit back will allow to \emph{call} (\ie\ identify) what is the
new nucleotide that each cluster has incorporated. The signal intensity, along to
its wave-length are recorded (both of them are needed for the base calling
process for the base call itself and its associated accuracy). Finally,
the fluorescent tags and the 3' caps are cleaved
and washed away so a new cycle can happen.
The number of cycles determines the final length of the reads.

Unfortunately, as the sequencing proceeds, the error rate of the sequencers
increases. This is due to the incomplete removal of the fluorescent signal which
increases the background noise and thus reduces the signal-to-noise ratio.

Once the programmed read length is achieved (typically between 25 to 200 nt),
the reads are washed away (after denaturation).

\NB The below steps are specific to the \emph{paired-end} protocol.

Then, after introduction of a new primer, the first index is sequenced
following the same sequencing cycle.
Once completed, the index read is washed off and the 3' end primer is deprotected.
Then, the \gls{DNA} fragment bends over and hybridises to a complimentary
oligonucleotide at the surface of the flow cell.
Next, the second index is sequenced and its product read washed away,
a single new bridge replication follows.
The new double-stranded \gls{DNA} fragment is denatured and the 3' primers are
protected before the forward strand is cleaved and washed away.
Finally, the reverse strand is sequenced following the previously described
sequencing cycle. Once the same number of sequencing cycles than the forward
strand is reached, the read product of the reverse strand is washed away.

\subsubsection{From analogous input to digital output}

At the end of the sequencing process, the set of images (one per sequencing cycle)
is produced. While it is possible to work with the images themselves, in most
cases, the sequencing facilities will perform the base calling and provide the
end-user with text files. In single-end sequencing, there is one file per sample.
In paired-end sequencing, the reads are separated based on their associated
indexes into two ordered files: all the reads from the forward strands (one end)
are grouped in one file, and the ones from the reverse strands (other end)
in another.

These files are usually distributed in \fastq\ format
\mycite{fastqFormat} which record for each cluster (read) a unique identifier,
a nucleotide sequence and a \gls{Phred} quality score for each base of the
sequence. A few optional information can also be provided (\eg\
the position of the read on the \gls{flow} (See \cref{sec:fastq_format} for
a random read example).

The \gls{Phred} quality score ($Q$) measures the accuracy of the identification
of the nucleobase to which it refers. These score are set by the base calling
program and are defined as $Q = -10\log_{10}(P)$ with $P$ the probability of
the base being called wrongly. There are several possible encoding formats
(See \cref{sec:PhredScore}).

\subsubsection{Data analysis}

\NB \Cref{fig:pipelineTrans} present an example of the overall \latin{in silico}
process of raw \Rnaseq\ data. It summarises the steps I used to process the data
I used within this thesis.

Before any downstream analysis, for each read, the genomic region (or
\emph{locus}) from which it has been originally expressed need to be identified.
Indeed, \Rnaseq\ main objective is to quantify the expression of genomic
\emph{features}\footnote{These genomic features could be genes, isoforms,
exons, novel genes, \dots\
In short, any genomic region with an annotated function.}. In other words,
the transcriptome needs to be reconstruct from the short reads and annotated
(\ie\ identify which features have been expressed in each library).

Two main different strategies (see further `Reconstruction strategies' segment)
manage to accomplish this identification step. Independently of the
approach, this step is the most challenging and time consuming
of the workflow. Tools, that tackle the reconstruction, usually provide a
number of tunable heuristic parameters (\eg\ maximum number of allowed mismatches
or indels per read before discarding a possible identification\footnote{Indeed,
many reads will have many identifications, these reads are defined as
\emph{ambiguous reads}.})
to speed up the task.
Unfortunately, as on Illumina platforms, the base calling accuracy decreases
along the read length, this may lead to an information loss \mycite{TrimRNAseq}.
To prevent informative reads to be discarded, it is opportune to perform a quality
check of the raw data prior to the identification step. Thus, reads with a drop of
accuracy in their 3'end may be shortened (\ie\ trimmed) and rescued for the next
reconstruction step. Similarly, low quality reads may be discarded hence
lowering the complexity
of the reconstruction task and hasten its accomplishment.

\minisec{Quality check, trimming and filtering}\label{minisec:trim}

The quality assessment allows to remove any read (or part of it) that would
increase the complexity of the reconstruction step or skew the downstream analyses.

It is wise to discard uninformative reads, \ie\
reads with a low sequence complexity (\eg\ poly-T or poly-A tails) or with
uncalled base (N). Indeed, these reads will hamper the processing time as they
usually map to several parts of the genome while also decreasing the accuracy of
the global gene expression estimations.

For similar reasons, it is judicious to remove reads with a low overall quality
score\footnote{It may vary based on the complete set of reads to analyse.}.

It is also prudent to check and remove any read that may map to possible
contamination sources\footnote{For example, for eukaryotes, by aligning (see next
segment) every read to the \species{Escherichia coli} genome}).
Indeed, as these reads are ambiguous, it is safer
to discard them than skew the expression estimations.

Finally, as a number of tools (mappers in particular) only accept reads
with an unique length, the purity-length balance requires optimisation.
Indeed, the trimming has to compromise between
an approach too lenient (where many unfit reads are discarded
by the mappers at a later step) and
a too stringent one (where too little reads are left for a pertinent analyses or
where shorter reads increase the overall complexity and therefore hinder
the mapping both on time and accuracy \mycite{Trimwisely}).

\NB Generally, after the sequencer calls the reads, a first trimming removes
all the adaptors and barcodes needed by the sequencing protocols. Thus,
in principle, they are not to be found in the ``raw data''.
However, to avert any latter contingency, a research against
a list of the most common adaptors and an over-representation assessment of small
sequences (\emph{k-mers}\footnote{\emph{k-mer}: All possible subsequences of length
\emph{k} of a \emph{read}.}) at each end of the reads is good practice.


\minisec{Reconstruction strategies}

Two main approaches can be used for the very computationally expensive step of
identification. I will present them in a decreasing order of complexity:
the \latin{de novo} assembly of the reads and then the Reads alignment approach
(to a genome reference or a transcriptome one).

Regardless of the approaches, the reconstructed transcriptome is usually reported
as a \emph{SAM} file \mycite{SAMformat} (or one of its derivative format either
\emph{BAM} or more recently \emph{CRAM}).

\begin{figure}
    \includegraphics[scale=0.60]{introduction/MappingStrategies_cropped.pdf}\centering
    \caption[Overview of main reconstruction strategies for
    \Rnaseq\ transcriptome]{\label{fig:OverviewRnaseqMapping}\textbf{Overview of main
    reconstruction strategies for a \Rnaseq\ transcriptome}}
\end{figure}


\subminisec{\latin{de novo} Assembly}
This approach is favoured when the reference genome of the species of interest
is unavailable or of poor quality
(\eg\ many non-model organisms) or inadequate (\eg\ cancer samples)
for the samples of interest. However, if a reference already exists this strategy
is avoided to the utmost.

It allows the unbiased discovery of novel
exon-exon junctions \mycite{deBruijn}. As none of the datasets I use in this
thesis has been reconstructed through this approach, I briefly summarise
the main points below as more in-depth reviews cover this strategy
\mycite{denovoReview}.

In \latin{de novo} assembly, the reconstruction of the transcriptome happens
with the construction of the longest possible \emph{contigs} (\ie\ contiguously
expressed regions) based on sets of overlapping reads. Short length reads add
to the overall complexity of this approach. While paired-end reads may help to
solve a number of genomic regions, lowly expressed or repetitive regions remain
challenging to determine. There are several algorithmic approaches for \latin{de
novo} transcriptome assembly \mycite{algorithmsDenovo},
though the most prevalent one is the de Bruijn representation \mycite{deBruijn}.


\subminisec{Read alignment}
This approach exploits prior knowledge. The reads are aligned to a reference to
hasten the reconstruction process. The reference may be a genome or a
transcriptome (provided that a good annotation is available).

\subsubminisec{Genome reference}
Aligning to the genome allows to discover new genes or isoforms. However, it
requires a splice-aware algorithms, \ie\ they need to align the
reads across the splice-junctions (which is possible but non-trivial).
As illustrated in \Cref{fig:OverviewRnaseqMapping} --- Genome alignment part,
the reads might span a number of discontinued regions of the reference.
While on one hand, aligning to the genome avoids \emph{multiple mapping
issues}\footnote{Due to sequence similarity, a same read or subpart of a read
may be attributed to a number of different loci in the genome. As it is
impossible to directly attribute the read to its original locus of expression,
distribution models have to be considered to avoid unnecessary skewness while
the quantification step.} for a same exon. Indeed, irrespectively of the number
of a gene isoforms including an exon, the sequence of this exon is transcribed
only once in the reference. However, this also implies that, for correct
quantifications of the different isoforms, the genome needs to provide the
coordinates for the isoforms and that an accurate quantification at isoform levels
requires further analysis.


\subsubminisec{Transcriptome reference}
Using a transcriptome as reference instead of a genome reduces the complexity
of the aligning step due to the lack of intronic sequences. However, it also
limits the potential downstream analyses, \eg\ any new (or unannotated) gene
or isoform will be missed. This approach is the easiest, but a pre-existing
accurate and well-annotated gene models is required. The third part of the
\Cref{fig:OverviewRnaseqMapping} shows in fact that this approach is simpler
to the previous one as a direct alignment of the reads is done against the
transcriptome of reference. This enables to quantify accurately the expression
of gene isoforms, provided that the gene model is correct and the reads may be
attributed unambiguously to a single isoform for each gene.


To mitigate between very computational greedy approaches and more constraining
ones, several tools complementary use the previous strategies.

\subminisec{Hybrid approach between \latin{de novo} and alignment}
There are tools like \soft{TopHat2}\footnote{\soft{TopHat} ---
\href{https://ccb.jhu.edu/software/tophat/index.shtml}%
{https://ccb.jhu.edu/software/tophat/index.shtml}} (2.0.12) \mycite{tophat2}
(which has been used to reconstruct all the
transcriptomes involved in this thesis --- see \Cref{ch:datasets}),
that uses an hybrid approach between a reference alignment and a \latin{de novo}
assembly. As a first (but optional) step, \soft{TopHat2} tries to contiguously
align the reads with \soft{Bowtie}\footnote{\soft{Bowtie} ---
\href{http://bowtie-bio.sourceforge.net/index.shtml}%
{http://bowtie-bio.sourceforge.net/index.shtml}} \mycite{Bowtie}
to the reference transcriptome when this one is provided. Then
(or alternatively as a first step), the unmapped reads are aligned to the
reference genome. The intention in these alignment steps is to identify and
localise the exons, which may eventually span over splicing junctions and be
connected through spliced alignment. Then, \soft{TopHat2} builds a database of
possible exon-exon splice junctions from the set of mapped reads and then split
the remaining unmapped (or aligned with a low score) reads in order to find
possible new split events via a seed-and-extend approach. Potential new events
are considered when smaller segments, that are matching exactly the reference
in their good quality score regions, are also overlapping a splice junction
reported in the newly created database.

The smaller segments
have to match exactly for their regions with good quality score with the reference
and have overlaps (for an user defined length) with a splice junction.
These segment sections are called seed.
Any read with a seed is then checked for a
complete alignment to the exons on both side of the junction.

In the case of paired-end data, each read is first processed separately. Although,
they are a valuable resource in a final evaluation phase, where with additional
information sources, they help to determinate among the many possibilities which
are the most credible ones.


\minisec{Quantification of \emph{features}}




\rew{Angela:
and several others. The result of the combination of all these parameters is
complex and species dependent and therefore would be difficult to assess even
with simulated data. To my knowledge this has not been done and it is unclear
what the impact of changing the parameters is. What limited comparisons exist
have been reported by competing alignment tools in scenarios of limited scope,
for example in [60] and [164].\\\\
Alignment to the genome results in a set of one or more genomic coordinates for
each aligned read which may or may not span exon junctions. By itself this
information is of limited use so alignments can be further matched to known
annotated features (for example by searching for overlaps between aligned reads
and genes) or they can be used to build gene models de novo. One of the earliest
and most well known programs to achieve the latter is a software application
called Cufflinks [146]. In this program the authors have implemented an algorithm
which tries to find the smallest possible set of transcripts that explains all
observed (aligned) read or read pairs (henceforth in this section referred to
as fragments). As in the de novo assembly approach described above, the fragments
are used to find islands of expression. The problem of assembly is then to
construct a directed acyclic graph in which fragments are nodes and pairs of
nodes are connected if the fragments overlap with one another and if they do not
imply splice junctions which cannot be present simultaneously in the same
transcript (i.e. if the aligned reads are not incompatible, Figure 1.7). Assembly
with Cufflinks is done independently for each island.
Finally, Cufflinks finds the minimum number of partitions into chains of the
graph by implementing a proof of a theorem for the decomposition of partially
ordered sets known as Dilworth’s theorem. This set of minimum number of paths
however may not be unique as in the example in Figure 1.8. In order to “phase”
distant exons together Cufflinks chooses the path that minimises the total cost
obtained by weighting each graph edge between nodes x and y with a cost C(x, y)
= − log(1 − |φx − φy |), where φx and φy are the percent-spliced-in metrics
computed for x and y by dividing the number of alignments compatible with x or y
by the total number of fragments overlapping x or y and normalising for the
length of x or y.
The method employed by Cufflinks requires high coverage (high read overlap) to
create good gene models, otherwise lowly expressed transcripts will be broken
into pieces. Still, at low coverage this method is more sensitive than the de
novo assemblers of the previous sections [134]. One other caveat of this approach
is that paths are maximally extended as in the example of Figure 1.7, making it
impossible to detect some instances of alternative transcript start and end
sites. Finally, Cufflinks finds the minimum set of transcripts that explains the data.
Choosing the simplest model over a more complex one when both models are equally
valid is an application of Occam’s Razor. However, while Occam’s Razor is of great
use for the development of theoretical models, its adoption for arbitrating
between models in biology is controversial since the result of evolution is not
necessarily optimal [157].
\\\\
For contiguous alignment to a reference, the most well known aligner is Bowtie.
Bowtie indexes and compresses a genome sequence using a technique called a
Burrows-Wheeler (BW) transform. The BW transformation allows the index to be
held in memory and is faster than other aligning approaches such as spaced seed
indexing [144]. The output of Bowtie, as with most current aligners including all
previously mentioned, is in the SAM format. Among other information the SAM format
provides for each read one or more alignment records describing one or more
locations, in this case a coordinate along a feature, to which the read aligns [85].}



\subsection{Proteome exploration with Mass spectrometry}


\begin{comment}
\subsection{Proteomics}
    \subsubsection{Main technologies}
    \subsubsection{Mass sprectrometry}

\end{comment}

\section{Big studies, big data and Reproducibility}



\subsection{Reproducibility issues // experimental design}
    \subsubsection{Batch effect}\label{subsub:BatchEffect}
    Batch effect: \mycite{batchEffect}

    \begin{itemize}
            \item{Different samples}
            \item{Technology: wet lab but also software: Rupgrade, \ldots}
            \item{missing meta-data}
        \end{itemize}
    \subsection{Main concerns}
        \subsubsection{Detection}
        \subsubsection{Quantification}
    \subsection{Consistency through biological layers}

\mycite{reviewRNAseqBestpractice}
\subsection{Co-studies on Transcriptomics and Proteomics in the literature}
\subsection{EBI Expression Atlas or how integrating independent datasets }




\section{Reproducibility vs. Repeatability}


\section{Experimental design}
\subsection{Technical replicates}
\subsection{Biological replicates}
pourquoi c'est important




\section{discussion or conclusion}


\Rnaseq\ as \Dnaseq\ needs many corrections as to prevent biases in the downstream
analysis. However, due to the dynamic component of the transcriptome
contrarily to the genome, while it is possible to apply the corrections in
\Dnaseq\ and then analyse the data \mycite{dnaseqCorr},
in \Rnaseq, corrections are already part of
the analysis and requires often as much skills than flair. It is possible that
future protocols will overcome this issue.


\section{Aims of the thesis}

One main focus of my work for this thesis was to appraise the
consistency of findings that have been enlightened in normal Human tissues by
individual large scale transcriptomics and, more recently, large-scale
proteomics studies.

While the amount of data to process and integrate can be challenging,
paradoxically the data available to draw statistically significant conclusions
is often still too sparse. However, we do have enough to provide at least a
qualitative assessment of the consistency and evaluate the feasibility of a
quantitative assessment in general, if not, realise a quantitative integration
for a few key tissues.

\section{Achievements}
\begin{itemize}
    \item First time so many datasets have been reprocessed together with the
        same annotation and hence comparison are better (y'a le truc de chez Burges,
        mais ils se sont concentré sur une petite partie des genes)
    \item surtout comparison of Gtex and Uhlen (aussi approfondi)
    \item reprocessing of all the proteome untargeted in human
    \item comparison entre protein expression and 2 datasets (meilleur que le
        science report dans le sens où il y a pas de polémique
    \item siteweb//application de visualisaiton de la comparison
    \item liste croisé de tissues specifique et house keeping à travers datasets
        transcriptomic et protoemic
    \item attempt de normalisation basée sur les housekeepings
\end{itemize}
